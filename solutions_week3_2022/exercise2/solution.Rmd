---
title: "Censored observations"
subtitle: "Week3-ex2, solution"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercise instructions

This is the same exercise as 2.1 except that now the posterior inference is performed with MCMC using Stan. Hence, instead of the grid based approximation we use Monte Carlo approximation to do the same analyses as in exercise 2.1.

Suppose you have a $\text{Gamma}(\alpha=1,\beta=1)$ prior distribution on the parameter $\lambda$ which corresponds to the expected number of ship ice besetting events (=events where a ship gets stuck in ice) during 1000 nautical miles in ice infested waters. The number of besetting events, $y$ per distance $d$ (nm) is modeled with a Poisson distribution $\text{Poisson}(\lambda \times d)$.
The hyper-parameter $\alpha$ is the shape and $\beta$ is the inverse scale parameter. You are told that during winters 2013-2017 category A ice breakers traveled in total 6560 nautical miles in the Kara Sea (a sea area in the Arctic Sea). Within this distance they experienced in total more than 2 but less than 7 ice besetting events. 


\begin{itemize}
\item[1)] Implement the model with Stan and sample from the posterior of $\lambda$.
\begin{itemize}
\item[a)] Check for convergence visually and by calculating the PSRF statistics.
\item[b)] Calculate the autocorrelation of the samples.
\end{itemize}
\item[2)] Using the samples of $\lambda$ 
\begin{itemize}
\item[a)] draw the posterior density function of $\lambda$ and 
\item[b)] calculate the posterior probability that $\lambda<1$ and the 5\% and the 95\% quantiles.
\item[c)] calculate the posterior mean and variance of $\lambda$.
\end{itemize}
\item[3)] Draw samples from the posterior predictive distribution for new $\tilde{y}$ for a ship traveling 1500 nm distance and 
\begin{itemize}
\item[a)] draw histogram of samples from the posterior predictive distribution for $\tilde{y}$
\item[b)] Calculate the posterior predictive mean and variance of $\tilde{y}$
\end{itemize}
\end{itemize}


# Model answer

## 1-3)

The posterior probability density function in case of censored observation is 

$p(\lambda|2<y<7,d=6.56) \propto \left(\text{Poisson}(3|\lambda\times d)+\text{Poisson}(4|\lambda\times d)+\text{Poisson}(5|\lambda\times d)+\text{Poisson}(6|\lambda\times d)\right)\text{Gamma}(\lambda|1,1)$

When using Stan, we need to first load the needed libraries into R and define a Stan model

```{r}
library(ggplot2)
library(StanHeaders)
library(rstan)
library(coda)
set.seed(123)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

censored_observations_model="
data{
  real<lower = 0> d;
  int<lower = 0> y1;
  int<lower = 0> y2;
}
parameters{
  real<lower = 0> lambda;
}
model{
  lambda ~ gamma(1,1);  //prior
  target += log( poisson_cdf(y2,d*lambda)-poisson_cdf(y1,d*lambda) );
}
"
```

Now we can define the data list and run Markov chain with Stan

```{r}
dataset <- list ("d"=6.56, "y1"=2, "y2"=6)

#give initial values for all chains for parameter theta
init1 <- list (lambda = 0.1)
init2 <- list (lambda = 2.5)
init3 <- list (lambda = 1)
inits <- list(init1, init2, init3) 

# stan function does all of the work of fitting a Stan model and 
# returning the results as an instance of stanfit = post in our exercises.
post=stan(model_code=censored_observations_model,data=dataset,init=inits,
          warmup=500,iter=1000,chains=3,thin=1)
```

Next we can examine the posterior samples in various ways.

```{r}
# visual inspection 
plot(post, plotfun= "trace", pars ="lambda", inc_warmup = TRUE)

# Inspection with PSRF=Rhat
print(post,pars="lambda")

# Compared summary statistics for different chains
summary(post,pars="lambda")

#Calculate the autocorrelation of the samples after removing burn-in.  Is autocorrelation
#a problem here?
stan_ac(post,"lambda",inc_warmup = FALSE, lags = 25)
```


## 2)

```{r}
#plot histogram of the posterior of lambda (approximation for density function)
plot(post, plotfun = "hist", pars = "lambda",bins=50)

# take samples of lambda into a vector
lambda = as.matrix(post, pars="lambda")

# Calculate the probability that lambda<1
pr.1 = sum(lambda<1)/length(lambda)
print(pr.1)
post_summary2 <- quantile(lambda,probs = c(0.05, 0.95))
print(post_summary2)

#calculate the posterior mean and variance
print(mean(lambda))
print(var(lambda))
```

## 3

Next we draw samples from the posterior predictive distribution for new $\tilde{y}$

```{r}
y = matrix(,nrow=length(lambda),ncol=1)
for (i1 in 1:length(lambda)){
  y[i1] = rpois(1,1.5*lambda[i1])
}
# alternatively you can draw samples as follows
# y = rpois(lambda,1.5*lambda)

hist(y)
(mean.y = mean(y))
(var.y = var(y))

```



# Grading

**Total 10 points:** 4 points for correclty doing step 1. 3 points for correctly doing step 2. 3 points for correctly doing step 3. 

