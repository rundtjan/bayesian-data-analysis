---
title: "Censored observations"
subtitle: "Week3-ex2, solution"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercise instructions

This is the same exercise as 2.1 except that now the posterior inference is performed with MCMC using Stan. Hence, instead of the grid based approximation we use Monte Carlo approximation to do the same analyses as in exercise 2.1.

Suppose you have a $\text{Gamma}(\alpha=1,\beta=1)$ prior distribution on the parameter $\lambda$ which corresponds to the expected number of ship ice besetting events (=events where a ship gets stuck in ice) during 1000 nautical miles in ice infested waters. The number of besetting events, $y$ per distance $d$ (nm) is modeled with a Poisson distribution $\text{Poisson}(\lambda \times d)$.
The hyper-parameter $\alpha$ is the shape and $\beta$ is the inverse scale parameter. You are told that during winters 2013-2017 category A ice breakers traveled in total 6560 nautical miles in the Kara Sea (a sea area in the Arctic Sea). Within this distance they experienced in total more than 2 but less than 7 ice besetting events. 


\begin{itemize}
\item[1)] Implement the model with Stan and sample from the posterior of $\lambda$.
\begin{itemize}
\item[a)] Check for convergence visually and by calculating the PSRF statistics.
\item[b)] Calculate the autocorrelation of the samples.
\end{itemize}
\item[2)] Using the samples of $\lambda$ 
\begin{itemize}
\item[a)] draw the posterior density function of $\lambda$ and 
\item[b)] calculate the posterior probability that $\lambda<1$ and the 5\% and the 95\% quantiles.
\item[c)] calculate the posterior mean and variance of $\lambda$.
\end{itemize}
\item[3)] Draw samples from the posterior predictive distribution for new $\tilde{y}$ for a ship traveling 1500 nm distance and 
\begin{itemize}
\item[a)] draw histogram of samples from the posterior predictive distribution for $\tilde{y}$
\item[b)] Calculate the posterior predictive mean and variance of $\tilde{y}$
\end{itemize}
\end{itemize}


## 1)

The posterior probability density function in case of censored observation is 

$p(\lambda|2<y<7,d=6.56) \propto \left(\text{Poisson}(3|\lambda\times d)+\text{Poisson}(4|\lambda\times d)+\text{Poisson}(5|\lambda\times d)+\text{Poisson}(6|\lambda\times d)\right)\text{Gamma}(\lambda|1,1)$

When using Stan, we need to first load the needed libraries into R and define a Stan model

```{r}
library(ggplot2)
library(StanHeaders)
library(rstan)
library(coda)
set.seed(123)

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

censored_observations_model="
data{
int<lower = 0> alpha;
int<lower = 0> beta;
real<lower = 0> distance;
}
parameters{
real<lower = 0> lambda;
}
model{
lambda ~ gamma(alpha, beta); //prior
target += log(poisson_cdf(6, lambda*distance)-poisson_cdf(2, lambda*distance));
}
"

```


Now we can define the data list and run Markov chain with Stan

```{r}
dataset <- list("alpha"=1, "beta"=1, "distance"=6.56)


#give initial values for all chains for parameter theta
init1 <- list(lambda=1)
init2 <- list(lambda=3)
init3 <- list(lambda=2)
inits <- list(init1, init2, init3)

# stan function does all of the work of fitting a Stan model and 
# returning the results as an instance of stanfit = post in our exercises.
post=stan(model_code=censored_observations_model,data=dataset,init=inits,
          warmup=500,iter=1000,chains=3,thin=1)
```


Next we can examine the posterior samples in various ways.

```{r}
# visual inspection 
plot(post, plotfun= "trace", pars ="lambda", inc_warmup = TRUE)
# Inspection with PSRF=Rhat
print(post,pars="lambda")
# Compared summary statistics for different chains
summary(post,pars="lambda")
#Calculate the autocorrelation of the samples after removing burn-in.  Is autocorrelation
#a problem here?
stan_ac(post,"lambda",inc_warmup = FALSE, lags = 25)

```
The result from the inspection gives us that there seems to be convergence and proper mixing in the Markov chains. The Rhat value of 1 shows the same thing numerically. Autocorrelation does not seem to be an issue in this case, as it is around zero with higher values of lag.


## 2)

First plotting the histogram:

```{r}
#plot histogram of the posterior of lambda (approximation for density function)
plot(post, plotfun = "hist", pars = "lambda",bins=50)
```
Then calculating probability that $\lambda<1$:
```{r}
# take samples of lambda into a vector
Nsamp=as.matrix(post, pars ="lambda")
# Calculate the probability that lambda<1
length(Nsamp[Nsamp < 1])/length(Nsamp)
```

Next up the 5% and 95% quantiles:
```{r}
# confidence interval or, i.e., quantiles for 5% and 95%:
quantile(Nsamp,probs=c(0.05,0.95))
```
And finally the mean and variance of $\lambda$:
```{r}
#calculate the posterior mean and variance
mean(Nsamp)
var(Nsamp)
```

## 3

Next we draw samples from the posterior predictive distribution for new $\tilde{y}$ using R:s random poisson generator:

```{r}

y_t = rpois(length(Nsamp), Nsamp*1.5)
hist(y_t)
```
Calculating mean and variance for the samples:
```{r}
mean(y_t)
var(y_t)

```





